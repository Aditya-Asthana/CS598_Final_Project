{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations (Report)\n",
    "Authors: Aditya Asthana, Krish Desai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchxrayvision\n",
      "  Downloading torchxrayvision-1.3.4-py3-none-any.whl (29.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.0 MB 51.6 MB/s eta 0:00:01 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./cs598env/lib/python3.8/site-packages (from torch) (4.13.2)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 87.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 35.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Using cached pillow-10.4.0-cp38-cp38-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./cs598env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./cs598env/lib/python3.8/site-packages (from matplotlib) (25.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl (247 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.57.0-cp38-cp38-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 108.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.7-cp38-cp38-macosx_10_9_x86_64.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 19.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0; python_version < \"3.10\"\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./cs598env/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting imageio\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "\u001b[K     |████████████████████████████████| 315 kB 108.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=1\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting scikit-image>=0.16\n",
      "  Downloading scikit_image-0.21.0-cp38-cp38-macosx_10_9_x86_64.whl (12.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.9 MB 91.4 MB/s eta 0:00:01    |██▊                             | 1.1 MB 91.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl (14 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./cs598env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./cs598env/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.20.2)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 8.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 37.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting tifffile>=2022.8.12\n",
      "  Downloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 50.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lazy_loader>=0.2\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-macosx_10_13_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 49.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.8\n",
      "  Using cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\n",
      "Installing collected packages: networkx, MarkupSafe, jinja2, mpmath, sympy, fsspec, filelock, torch, pillow, numpy, torchvision, tzdata, pytz, pandas, contourpy, fonttools, kiwisolver, importlib-resources, cycler, matplotlib, imageio, tqdm, idna, charset-normalizer, certifi, urllib3, requests, tifffile, lazy-loader, PyWavelets, scipy, scikit-image, torchxrayvision\n",
      "Successfully installed MarkupSafe-2.1.5 PyWavelets-1.4.1 certifi-2025.4.26 charset-normalizer-3.4.2 contourpy-1.1.1 cycler-0.12.1 filelock-3.16.1 fonttools-4.57.0 fsspec-2025.3.0 idna-3.10 imageio-2.35.1 importlib-resources-6.4.5 jinja2-3.1.6 kiwisolver-1.4.7 lazy-loader-0.4 matplotlib-3.7.5 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 pandas-2.0.3 pillow-10.4.0 pytz-2025.2 requests-2.32.3 scikit-image-0.21.0 scipy-1.10.1 sympy-1.13.3 tifffile-2023.7.10 torch-2.2.2 torchvision-0.17.2 torchxrayvision-1.3.4 tqdm-4.67.1 tzdata-2025.2 urllib3-2.2.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/adityaasthana/uiuc-mcs/CS598_Final_Report/cs598env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pandas matplotlib torchxrayvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pseudo-Labeling with TorchXRayVision\n",
    "We’ll load a pretrained DenseNet121 (CheXpert-trained), run inference on each image, and threshold the “Pneumonia” score at 0.5 to create binary labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityaasthana/uiuc-mcs/CS598_Final_Report/cs598env/lib/python3.8/site-packages/torchxrayvision/utils.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# **Cell 2: Imports & Globals**\n",
    "# --------------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import torchxrayvision as xrv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "IMAGE_DIR = Path(\"data_sources/images\")            # your raw scans\n",
    "MANIFEST_DIR = Path(\"outputs\")           # where CSVs will go\n",
    "MANIFEST_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Revised Cell 3: Load Pretrained Model & Preprocessor (CPU, 1-channel)**\n",
    "import torchxrayvision as xrv\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load TorchXRayVision DenseNet-121 (grayscale input)\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-chex\")\n",
    "model = model.eval()  # keep on CPU; no .cuda()\n",
    "\n",
    "# Preprocessing: resize → center-crop → grayscale → to-tensor → normalize\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=1),     # ensure 1 channel :contentReference[oaicite:0]{index=0}\n",
    "    transforms.ToTensor(),                           # yields shape [1,224,224]\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229]), # single-channel norms :contentReference[oaicite:1]{index=1}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_sources/images/00000001_000.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_sources/images/00000001_001.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_sources/images/00000001_002.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_sources/images/00000002_000.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_sources/images/00000003_000.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_path  label\n",
       "0  data_sources/images/00000001_000.png      1\n",
       "1  data_sources/images/00000001_001.png      1\n",
       "2  data_sources/images/00000001_002.png      1\n",
       "3  data_sources/images/00000002_000.png      1\n",
       "4  data_sources/images/00000003_000.png      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **Revised Cell 4: Generate Pseudo-Labels (1-channel inputs)**\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "records = []\n",
    "for fname in sorted(os.listdir(IMAGE_DIR)):\n",
    "    path = IMAGE_DIR / fname\n",
    "    img = Image.open(path).convert(\"L\")                # open as grayscale :contentReference[oaicite:2]{index=2}\n",
    "    tensor = preprocess(img).unsqueeze(0)              # shape [1,1,224,224]\n",
    "    with torch.no_grad():\n",
    "        scores = model(tensor).numpy().squeeze()       # outputs [21] pathology logits\n",
    "    idx = model.pathologies.index(\"Pneumonia\")\n",
    "    prob = torch.sigmoid(torch.tensor(scores[idx])).item()\n",
    "    label = int(prob >= 0.5)\n",
    "    records.append({\"image_path\": str(path), \"label\": label})\n",
    "\n",
    "df_labels = pd.DataFrame(records)\n",
    "df_labels.to_csv(MANIFEST_DIR/\"pseudo_labels.csv\", index=False)\n",
    "df_labels.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Two Domains\n",
    "- **Domain A:** your original images  \n",
    "- **Domain B:** apply a consistent augmentation (brightness & blur) to simulate a second “dataset”  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./cs598env/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./cs598env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/adityaasthana/uiuc-mcs/CS598_Final_Report/cs598env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# **Cell 5: Define Augmentation & Build Manifests**\n",
    "# --------------------------------------------------------------\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "    transforms.GaussianBlur(5),\n",
    "])\n",
    "\n",
    "# prepare lists\n",
    "manifests = {\"A\": [], \"B\": []}\n",
    "\n",
    "for _, row in df_labels.iterrows():\n",
    "    path, lbl = row[\"image_path\"], row[\"label\"]\n",
    "    # Domain A record\n",
    "    manifests[\"A\"].append({\"image_path\": path, \"label\": lbl, \"domain\": 0})\n",
    "    # Domain B: save augmented copy to a temp folder\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    aug = aug_transform(img)\n",
    "    save_path = IMAGE_DIR/\"domainB\"/os.path.basename(path)\n",
    "    save_path.parent.mkdir(exist_ok=True)\n",
    "    aug.save(save_path)\n",
    "    manifests[\"B\"].append({\"image_path\": str(save_path), \"label\": lbl, \"domain\": 1})\n",
    "\n",
    "!pip install scikit-learn\n",
    "# Split each into train/val/test (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "for dom, recs in manifests.items():\n",
    "    df = pd.DataFrame(recs)\n",
    "    train, temp = train_test_split(df, stratify=df.label, test_size=0.2, random_state=0)\n",
    "    val, test = train_test_split(temp, stratify=temp.label, test_size=0.5, random_state=0)\n",
    "    train.to_csv(MANIFEST_DIR/f\"{dom}_train.csv\", index=False)\n",
    "    val.to_csv(MANIFEST_DIR/f\"{dom}_val.csv\", index=False)\n",
    "    test.to_csv(MANIFEST_DIR/f\"{dom}_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dataset & DataLoader\n",
    "Define a PyTorch `Dataset` that reads our CSVs and returns `(img, label, domain)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Cell 6: Dataset Class**\n",
    "# --------------------------------------------------------------\n",
    "class ChestDataset(Dataset):\n",
    "    def __init__(self, manifest_csv, transform=None):\n",
    "        self.df = pd.read_csv(manifest_csv)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "        ])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.image_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, row.label, row.domain\n",
    "\n",
    "def make_loader(dom, split, bs=32, shuffle=True):\n",
    "    path = MANIFEST_DIR/f\"{dom}_{split}.csv\"\n",
    "    return DataLoader(ChestDataset(path), batch_size=bs, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model & ERM Training\n",
    "We’ll sample equally from each domain per step (“infinite loader” logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Cell 7 (CPU only): Model Factory & Training Loop**\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_model():\n",
    "    m = models.densenet121(pretrained=False)\n",
    "    m.classifier = nn.Linear(m.classifier.in_features, 1)\n",
    "    return m  # CPU model\n",
    "\n",
    "def train_erm(domains, max_steps=5000, lr=1e-4):\n",
    "    loaders = {d: make_loader(d, \"train\") for d in domains}\n",
    "    iters = {d: iter(loaders[d]) for d in domains}\n",
    "    model = make_model()            # CPU\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        for d in domains:\n",
    "            try:\n",
    "                imgs, labs, _ = next(iters[d])\n",
    "            except StopIteration:\n",
    "                iters[d] = iter(loaders[d])\n",
    "                imgs, labs, _ = next(iters[d])\n",
    "            # no .cuda()\n",
    "            preds = model(imgs).squeeze()\n",
    "            loss = crit(preds, labs.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        if (step+1) % 1000 == 0:\n",
    "            print(f\"Step {step+1}/{max_steps}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Worst-Group Accuracy\n",
    "Compute min accuracy over the four subgroups (domain×label).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Cell 8 (CPU only): Evaluation Metric**\n",
    "import torch\n",
    "\n",
    "def worst_group_acc(model, domains):\n",
    "    allP, allL, allD = [], [], []\n",
    "    for d in domains:\n",
    "        loader = make_loader(d, \"test\", bs=64, shuffle=False)\n",
    "        for imgs, labs, dom in loader:\n",
    "            with torch.no_grad():\n",
    "                out = torch.sigmoid(model(imgs)).round()  # CPU\n",
    "            allP.append(out)\n",
    "            allL.append(labs)\n",
    "            allD.append(dom)\n",
    "    P = torch.cat(allP)\n",
    "    L = torch.cat(allL)\n",
    "    D = torch.cat(allD)\n",
    "    accs = []\n",
    "    for d_val in torch.unique(D):\n",
    "        for c in [0,1]:\n",
    "            mask = (D==d_val)&(L==c)\n",
    "            accs.append((P[mask]==L[mask]).float().mean().item())\n",
    "    return min(accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Run Experiments & Plot\n",
    "Compare single-domain (A) vs. multi-domain (A+B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000/5000\n",
      "Step 2000/5000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/adityaasthana/uiuc-mcs/CS598_Final_Report/data_sources/images/00001088_015.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# **Cell 9: Execute & Compare**\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Single-domain\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m modelA \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_erm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m wgA \u001b[38;5;241m=\u001b[39m worst_group_acc(modelA, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Multi-domain\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain_erm\u001b[0;34m(domains, max_steps, lr)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m domains:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         imgs, labs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miters\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         iters[d] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(loaders[d])\n",
      "File \u001b[0;32m~/uiuc-mcs/CS598_Final_Report/cs598env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/uiuc-mcs/CS598_Final_Report/cs598env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/uiuc-mcs/CS598_Final_Report/cs598env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/uiuc-mcs/CS598_Final_Report/cs598env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mChestDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     14\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m---> 15\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, row\u001b[38;5;241m.\u001b[39mlabel, row\u001b[38;5;241m.\u001b[39mdomain\n",
      "File \u001b[0;32m~/uiuc-mcs/CS598_Final_Report/cs598env/lib/python3.8/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/adityaasthana/uiuc-mcs/CS598_Final_Report/data_sources/images/00001088_015.png'"
     ]
    }
   ],
   "source": [
    "# **Cell 9: Execute & Compare**\n",
    "# --------------------------------------------------------------\n",
    "# Single-domain\n",
    "modelA = train_erm([\"A\"])\n",
    "wgA = worst_group_acc(modelA, [\"A\"])\n",
    "# Multi-domain\n",
    "modelAB = train_erm([\"A\",\"B\"])\n",
    "wgAB = worst_group_acc(modelAB, [\"A\",\"B\"])\n",
    "\n",
    "print(f\"Single-domain worst-group acc: {wgA:.3f}\")\n",
    "print(f\"Multi-domain worst-group acc:   {wgAB:.3f}\")\n",
    "print(f\"Δ (multi – single):             {wgAB - wgA:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Cell 10: Visualization**\n",
    "# --------------------------------------------------------------\n",
    "plt.bar([\"Single\",\"Multi\"], [wgA, wgAB], color=[\"C0\",\"C1\"])\n",
    "plt.ylabel(\"Worst-Group Accuracy\")\n",
    "plt.title(\"When More Is Less: Single vs. Multi Domain\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "If Δ < 0, you’ve demonstrated that adding Domain B hurts the worst-group accuracy—i.e., “more is less,” reproducing the core finding from Compton et al.\n",
    "\n",
    "Feel free to tweak:\n",
    "- `max_steps`, learning rate, batch sizes  \n",
    "- Pseudo-label threshold or pathology choice  \n",
    "- Augmentation strength for Domain B  \n",
    "\n",
    "to see how robust the phenomenon is in your own data!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs598env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
